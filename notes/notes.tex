\documentclass[10.5pt]{article}
\usepackage{amsmath,textcomp,amssymb,geometry,graphicx,enumerate}
\usepackage{listings}
\usepackage{color}
\usepackage{svg}
\def\dx{{\rm dx}}
\def\tr{{\rm tr}}
\def\spa{{\rm span}}
\def\T{{\mathbb{T}}}
\def\H{{\mathbb{H}}}
\def\K{{\mathbb{K}}}
\def\L{{\mathbb{L}}}
\def\M{{\mathbb{M}}}
\def\N{{\mathbb{N}}}
\def\P{{\mathbb{P}}}
\def\Q{{\mathbb{Q}}}
\def\R{{\mathbb{R}}}
\def\C{{\mathbb{C}}}
\def\Z{{\mathbb{Z}}}
\newcommand{\interior}[1]{%
  {\kern0pt#1}^{\mathrm{o}}%
}
\newcommand{\closure}{\overline}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=R,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

\def\Name{Ameet Rahane}  % Your name
\def\Session{Spring 2019}


\title{ASD/MF Project Notes}

\author{Ameet Rahane}



\newenvironment{qparts}{\begin{enumerate}[{(}a{)}]}{\end{enumerate}}
\def\endproofmark{$\Box$}
\def\bigO{\mathcal{O}}
\newenvironment{proof}{\par{\bf Proof}:}{\endproofmark\smallskip}

\textheight=9in
\textwidth=6.5in
\topmargin=-.75in
\oddsidemargin=0.25in
\evensidemargin=0.25in


\begin{document}
\title{Types of Reinforcement learning algorithm -- adaptive $\epsilon$ methods}
\author{Ameet Rahane}
\maketitle
\date{}


\section{Preface}


All of the following methods are ways I'm considering changing explore/exploit behavior within models in order to fit it to a biological agent. Something of notes is that most of these models are built to optimize for reward gained. This is not necessarily our goal. We want to find the best fit to a mouse. 


\section{$\epsilon$ Greedy}

With this method, the amount of exploration is globally controlled by a parameter $\epsilon\in [0,1]$ that denotes the probability of action selection. You choose actions based on the following equation: 
\begin{equation}
a_t = \begin{cases}
a_t^* & p = 1-\epsilon\\
\text{random} & p = \epsilon
\end{cases}
\end{equation}

This basically stochastically chooses between two choices -- pure randomness or pure maximization (this is probably a bit idealistic and won't fit mice that well).

\section{Softmax}
\begin{itemize}
  \item Bias exploration towards promising actions
  \item Softmax action selection methods grade action probabilities by estimated values
  \item Most Common is Boltzmann Distribution: 
  \begin{equation}
\pi(a|s) = \dfrac{e^{\dfrac{Q(s,a)}{\tau}}}{e^{\sum_{a'\in A}\dfrac{Q(s,a')}{\tau}}}
  \end{equation}
  \item Here $\tau$ is a temperature constant. That is $\tau \to \infty$ implies that $P=\dfrac{1}{|A|}$ and $T\to 0$ implies that it's greedy
\end{itemize}

\section{Value Difference Based Exploration}
This method extends $\epsilon-$greedy by adapting a state dependent exploration probability $\epsilon(s)$ instead of the classical hand tuning. The key idea is to consider the TD-error observed from value-function backups as a measure of the agent's uncertainty about the environment which directly affects the exploration probability. \\

Consider the RL framework where an agent interacts with a Markovian decision process. At each time step $t\in \N$, the agent is in a sertain state $s_t\in S$. After the selection of the action,$ a_t\in A(s_t)$, the agent receives a reward signal from the environment $r_{t+1} \in \R$ and is passed into a successor state. The decision which action $a$ is chosen in a certain state is characterized by a policy $\pi(s) = a$, which could also be stochastic $\pi(a|s) = p(a_t=a|s_t=s$. A policy that maximizes the cumulative reward is denoted as $\pi^*$. \\

A state action value denotes the expected cumulative reward $R_t$ for following $\pi$ by starting in state $s$ and selection action $a$: 
\begin{equation}
Q^\pi(s,a) = E_\pi \{R_t|s_t=s, a_t=a\} = E_\pi \{\sum_{k=0}^\infty \gamma^kr_{t+k+1}|s_t=s,a_t=a\}
\end{equation} 
$\gamma$ is a discount factor in $(0,1]$ for episodic learning tasks and $0<\gamma< 1$ for continuous learning tasks. \\

The basic idea of VDBE is to extend $\epsilon$ greedy by controlling a state dependent explorations probability $\epsilon(s)$ in dependence of the value function error instead of manual tuning. The following equatins adapt such desired behavior according to a softmax Boltzmann distribution of the value fuction estimates. This is performed after eadch learning step by: 
\begin{equation}
f(s,a,\sigma) = \dfrac{1-e^\frac{-|a\text{TD-Error}|}{\sigma}}{1+e^\frac{-|a\text{TD-Error}|}{\sigma}}
\end{equation}
Then, we update $\epsilon$ as follows: 
\begin{equation}
\epsilon_{t+1}(s) =\delta f(s_t, a_t, \sigma) + (1-\delta) \epsilon_t(s)
\end{equation}

$\sigma$  is a positive constant called inverse sensitivity and $\delta \in [0,1)$, a parameter determining the influence of the selection action on the exploration rate. An obvious setting for $\delta$ may be the inverse of the number of actions in the current state: $\delta = \dfrac{1}{|{\cal A}(s)|}$. Low inverse sensitivities cause full exploration even at small value changes. On the other hand, shigh inverse sensitivities cause a high level of exploration only at large value changes. In the limit, the exploration rate converges to zero as the Q-function converges, which results in pure greedy action selection. One drawback is the exploration actions are chosen uniformly distributed among all possible actions in the current state. 

\section{Adaptive control between $\epsilon$ Greedy and Softmax}
\subsection{Learning the $Q$ function by on and off policy methods}
Value functions are learned by sampling observations of the interactions between the agent and its environment. For this, the branch of temporal difference learing offers two commonly used alogrithm which are named Sarsa for on-policy control:
\begin{equation}
\Delta_{\text{Sarsa}} = [r_{t+1} + \gamma Q(s_{t+1}, a_{t+1}) - Q(s_t,a_t]
\end{equation}
\begin{equation}
Q(s_t,a_t) = Q(s_t, a_t) + \alpha \Delta_\text{Sarsa}
\end{equation}
and Q learning for off policy control: 
\begin{equation}
b^* = {\rm argmax}_{b\in {\cal A}(s_{t+1})}Q(s_{t+1}, b)
\end{equation}
\begin{equation}
\Delta_{\text{Qlearning}} = [r_{t+1}+\gamma Q(s_{t+1}, b^*) - Q(s_t,a_t)]
\end{equation}
\begin{equation}
Q(s_t,a_t = Q(s_t, a_t) + \alpha \Delta_{\text{Qlearning}}
\end{equation}
$\alpha$ is a stepsize parameter. The only difference in two algorithms is the inclusion of successor state information used for the evaluation of action $a_t$ taken in state $s_t$ while learning the value function. 
\subsection{VDBE-Softmax}
One way we can fix the issue with VDBE is to do a softmax step instead of sampling uniformly for the explore. This is as follows: 
\begin{equation}
\pi(s) = \begin{cases}
\text{Softmax: }\dfrac{e^{Q(s,a)/\tau}}{\sum_be^{Q(s,b)/\tau}} & \zeta < \epsilon \\
{\rm argmax}_{a\in {\cal A}(s)}Q(s,a) & \zeta \geq \epsilon
\end{cases}
\end{equation}

This adapts the state dependent exploration rate according to VDBE but selects random actions according to Softmax in the case of exploration. 

\section{Creating a probability distribution over the possible $q$ value and sampling that}

\section{Apply the Softmax at each step}

\section{Binary stochastic switching between greedy behavior and probabilistic sampling}

\section{Binary stochastic switching between greedy behavior and softmax}

\section{Adaptive Exploration using stochastic neurons}


\section{Meta learning of exporation and exploitation parameters}

\section{Gradient Algorithms for Exploration/Exploitation Trade-offs}

\section{Chunk based exploration/exploitation???}





\end{document}